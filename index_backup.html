<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mahesh Raj — Portfolio</title>
  <!-- Font Awesome for icons -->
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
    integrity="sha512-p4VfG6xXFYkRcOCvhuYmx2yxBM7BTuwnd+UfSn/XqFbT3MuME4iUZOSjJfzlRYnFT07QKNtzTB0kRz7MA66c4g=="
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
  />
  <style>
    /* Base styles */
    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen,
        Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
      background-color: #0a0a0a;
      color: #f5f5f5;
      line-height: 1.6;
    }
    a {
      color: #10b981;
      text-decoration: none;
    }
    a:hover {
      color: #6ee7b7;
    }
    /* Layout */
    .container {
      width: 90%;
      max-width: 1000px;
      margin: 0 auto;
      padding: 0 1rem;
    }
    header {
      position: sticky;
      top: 0;
      width: 100%;
      z-index: 100;
      backdrop-filter: blur(8px);
      background-color: rgba(15, 15, 15, 0.8);
      border-bottom: 1px solid #262626;
    }
    .header-inner {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 0.75rem 0;
    }
    nav a {
      margin-left: 1rem;
      font-size: 0.9rem;
      color: #d4d4d4;
    }
    nav a:hover {
      color: #10b981;
    }
    .hero {
      display: flex;
      flex-direction: column;
      align-items: flex-start;
      justify-content: center;
      padding: 3rem 0;
      gap: 2rem;
    }
    .hero-content {
      flex: 2;
    }
    .hero h1 {
      font-size: 2rem;
      margin-bottom: 1rem;
      font-weight: 700;
    }
    .hero h1 span {
      color: #10b981;
    }
    .hero p {
      max-width: 600px;
      color: #d1d5db;
    }
    .hero-info {
      margin-top: 1rem;
      font-size: 0.9rem;
      color: #a3a3a3;
    }
    .hero-image {
      flex: 1;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .hero-image img {
      /* Increase the profile picture size for a more prominent hero image */
      width: 200px;
      height: 200px;
      border-radius: 50%;
      object-fit: cover;
      border: 2px solid #374151;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.6);
    }
    @media (min-width: 768px) {
      .hero {
        flex-direction: row;
      }
      .hero h1 {
        font-size: 2.5rem;
      }
    }
    /* Sections */
    section {
      margin-top: 3rem;
    }
    section h2 {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-size: 1.5rem;
      color: #10b981;
      margin-bottom: 1rem;
    }
    .item {
      background-color: #111827;
      border: 1px solid #374151;
      border-radius: 0.5rem;
      padding: 1rem;
      margin-bottom: 1rem;
    }
    .item h3 {
      margin-top: 0;
      color: #f3f4f6;
      font-size: 1.1rem;
    }
    .item .sub {
      color: #9ca3af;
      font-size: 0.9rem;
    }
    .item ul {
      margin-top: 0.5rem;
      padding-left: 1.2rem;
    }
    .item ul li {
      margin-bottom: 0.4rem;
    }
    .chips {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
    }
    .chip {
      border: 1px solid #374151;
      border-radius: 9999px;
      padding: 0.25rem 0.75rem;
      font-size: 0.8rem;
      background: #1f2937;
      color: #d1d5db;
    }
    footer {
      margin-top: 4rem;
      padding: 2rem 0;
      border-top: 1px solid #262626;
      display: flex;
      flex-direction: column;
      gap: 1rem;
      font-size: 0.8rem;
      color: #9ca3af;
    }
    footer a {
      color: #10b981;
    }
    footer .footer-links {
      display: flex;
      gap: 1rem;
    }

    /* Animations */
    @keyframes fadeUp {
      0% {
        opacity: 0;
        transform: translateY(20px);
      }
      100% {
        opacity: 1;
        transform: translateY(0);
      }
    }
    /* This class will trigger the fade-up animation once applied to an element */
    .fade-up {
      opacity: 0;
      animation: fadeUp 0.8s ease-out forwards;
    }

    /* Expandable box styles */
    .expandable-item {
      cursor: pointer;
      transition: all 0.3s ease;
    }
    .expandable-item:hover {
      border-color: #10b981;
      box-shadow: 0 4px 12px rgba(16, 185, 129, 0.15);
    }
    .expand-icon {
      transition: transform 0.3s ease;
      margin-left: auto;
      color: #10b981;
    }
    .expanded .expand-icon {
      transform: rotate(180deg);
    }
    .item-header {
      display: flex;
      align-items: flex-start;
      justify-content: space-between;
    }
    .item-title-content {
      flex: 1;
    }
    .expanded-content {
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.4s ease, padding 0.4s ease;
      padding: 0 1rem;
    }
    .expanded .expanded-content {
      max-height: 800px;
      padding: 1rem;
    }
    .project-details {
      margin-bottom: 1.5rem;
      padding: 1rem;
      background-color: #1f2937;
      border-radius: 0.5rem;
      border-left: 3px solid #10b981;
    }
    .project-details h4 {
      color: #10b981;
      margin: 0 0 0.5rem 0;
      font-size: 1rem;
    }
    .project-details p {
      margin: 0;
      color: #d1d5db;
      line-height: 1.5;
    }

    /* Image slideshow styles */
    .image-slideshow {
      position: relative;
      width: 100%;
      height: 300px;
      border-radius: 0.5rem;
      overflow: hidden;
      background-color: #111827;
      border: 1px solid #374151;
    }
    .slideshow-container {
      position: relative;
      width: 100%;
      height: 100%;
    }
    .slide {
      display: none;
      width: 100%;
      height: 100%;
      position: relative;
    }
    .slide.active {
      display: block;
    }
    .slide img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .slide-caption {
      position: absolute;
      bottom: 0;
      left: 0;
      right: 0;
      background: linear-gradient(transparent, rgba(0, 0, 0, 0.8));
      color: white;
      padding: 1rem;
      font-size: 0.9rem;
    }
    .slideshow-nav {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      background: rgba(0, 0, 0, 0.6);
      color: white;
      border: none;
      padding: 0.5rem;
      cursor: pointer;
      border-radius: 0.25rem;
      transition: background-color 0.3s ease;
    }
    .slideshow-nav:hover {
      background: rgba(16, 185, 129, 0.8);
    }
    .slideshow-nav.prev {
      left: 10px;
    }
    .slideshow-nav.next {
      right: 10px;
    }
    .slideshow-dots {
      text-align: center;
      padding: 1rem 0;
    }
    .dot {
      height: 10px;
      width: 10px;
      margin: 0 3px;
      background-color: #6b7280;
      border-radius: 50%;
      display: inline-block;
      cursor: pointer;
      transition: background-color 0.3s ease;
    }
    .dot.active, .dot:hover {
      background-color: #10b981;
    }
  </style>
</head>
<body>
  <header>
    <div class="container header-inner">
      <a href="#top" style="font-weight: 700; color: #10b981; font-size: 1.25rem;">B. Mahesh Raj</a>
      <nav>
        <a href="#about">About</a>
        <a href="#experience">Experience</a>
        <a href="#research">Research</a>
        <a href="#projects">Projects</a>
        <a href="#skills">Skills</a>
        <a href="#education">Education</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <!-- Hero -->
      <section id="about" class="container hero fade-up" style="animation-delay:0s;">
    <div class="hero-content">
      <h1>
        Hi, I’m <span>Mahesh</span> — CSE undergrad passionate about deep learning, computer vision, and building practical AI systems.
      </h1>
      <p>
        Looking to collaborate on deep learning, computer vision, or generative AI. Open to internships and research opportunities.
      </p>
        <div class="hero-info">
          <!-- Green location icon with slight spacing before the city name -->
          <i class="fa-solid fa-location-dot" style="color:#10b981; margin-right:0.5rem;"></i>
          Coimbatore, India
        </div>
    </div>
    <div class="hero-image">
      <!-- Updated to use locally hosted profile image for reliability -->
      <img src="profile.png" alt="Mahesh Raj portrait" />
    </div>
  </section>

  <!-- Experience -->
      <section id="experience" class="container fade-up" style="animation-delay:0.2s;">
    <h2><i class="fa-solid fa-briefcase"></i> Experience</h2>
    <div class="item">
      <h3>SDE Intern — Finance Automation</h3>
      <div class="sub">Amazon &mdash; May 2024 – July 2024 · Bangalore, India</div>
      <ul>
        <li>Built a VQA + field-value extraction proof‑of‑concept for tax documents using LLMs/VLMs (Python, PyTorch, Bedrock, Textract, LoRA/QLoRA).</li>
        <li>Improved accuracy from 80–85% to 99% overall with 97% on critical fields.</li>
        <li>Shipped a data pipeline monitoring system (Lambda, Athena, DynamoDB, S3, EventBridge, CloudWatch, SQS/SNS, Docker) with automated alerting.</li>
      </ul>
    </div>
  </section>

  <!-- Research -->
      <section id="research" class="container fade-up" style="animation-delay:0.4s;">
    <h2><i class="fa-solid fa-flask"></i> Research</h2>
    
    <div class="item expandable-item" data-project="soil-moisture">
      <div class="item-header">
        <div class="item-title-content">
          <h3>Estimating Soil Moisture from Satellite Data</h3>
          <div class="sub">Amrita Vishwa Vidyapeetham × INRAE (France) &mdash; Mar 2024 – Jul 2025</div>
          <ul>
            <li>Remote sensing + ML for soil moisture prediction and plant life cycles.</li>
            <li>Advanced satellite data processing with PyTorch/TensorFlow.</li>
          </ul>
        </div>
        <i class="fa-solid fa-chevron-down expand-icon"></i>
      </div>
      <div class="expanded-content">
        <div class="project-details">
          <h4>Project Overview</h4>
          <p>This collaborative research project with INRAE (French National Research Institute for Agriculture, Food and Environment) focuses on developing advanced machine learning models to estimate soil moisture levels from satellite imagery. The project combines remote sensing techniques with deep learning to create predictive models that can monitor agricultural conditions and support sustainable farming practices.</p>
        </div>
        <div class="project-details">
          <h4>Technical Approach</h4>
          <p>Utilizing multi-spectral satellite data from Sentinel-1 and Sentinel-2 missions, we process radar and optical imagery to extract soil moisture indicators. Our deep learning pipeline includes CNN-based feature extractors, LSTM networks for temporal analysis, and ensemble methods for improved accuracy. The models are trained on ground truth data collected from various agricultural sites across different climatic conditions.</p>
        </div>
        <div class="project-details">
          <h4>Key Technologies</h4>
          <p>PyTorch for deep learning model development, TensorFlow for production deployment, Google Earth Engine for satellite data access, GDAL for geospatial processing, and Scikit-learn for traditional ML baseline comparisons. The project also incorporates cloud computing resources for large-scale data processing and model training.</p>
        </div>
        <div class="image-slideshow">
          <div class="slideshow-container">
            <div class="slide active">
              <img src="gallery/research/soil-moisture/image1.jpg" alt="Soil Moisture Study 1">
              <div class="slide-caption">Soil moisture analysis using satellite data</div>
            </div>
            <div class="slide">
              <img src="gallery/research/soil-moisture/image2.jpg" alt="Satellite Data Processing">
              <div class="slide-caption">Satellite data processing pipeline</div>
            </div>
            <div class="slide">
              <img src="gallery/research/soil-moisture/image3.jpg" alt="ML Model Results">
              <div class="slide-caption">Machine learning model results and validation</div>
            </div>
          </div>
          <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">❮</button>
          <button class="slideshow-nav next" onclick="changeSlide(this, 1)">❯</button>
          <div class="slideshow-dots">
            <span class="dot active" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
          </div>
        </div>
      </div>
    </div>

    <div class="item expandable-item" data-project="4d-mesh">
      <div class="item-header">
        <div class="item-title-content">
          <h3>Text‑Prompted 4D Mesh Character Animation using GNNs &amp; Diffusion</h3>
          <div class="sub">Amrita Vishwa Vidyapeetham &mdash; Nov 2024 – Aug 2025</div>
          <ul>
            <li>Latent graph diffusion to handle varying mesh topologies.</li>
            <li>Pipeline with GNN autoencoders + diffusion for text‑prompted 4D mesh generation.</li>
          </ul>
        </div>
        <i class="fa-solid fa-chevron-down expand-icon"></i>
      </div>
      <div class="expanded-content">
        <div class="project-details">
          <h4>Research Innovation</h4>
          <p>This cutting-edge research project focuses on generating temporally coherent 4D mesh animations directly from text prompts. The work addresses the challenge of creating realistic character animations that maintain mesh topology consistency across time while being controlled by natural language descriptions. This has significant applications in gaming, film production, and virtual reality.</p>
        </div>
        <div class="project-details">
          <h4>Methodology</h4>
          <p>Our approach combines Graph Neural Networks (GNNs) with diffusion models in a novel latent space representation. The GNN autoencoder learns compact latent representations of mesh geometries, while the diffusion model operates in this latent space to generate smooth temporal transitions. Text conditioning is achieved through CLIP embeddings that guide the diffusion process.</p>
        </div>
        <div class="project-details">
          <h4>Technical Contributions</h4>
          <p>Key innovations include a topology-aware latent space that preserves mesh connectivity, a temporal consistency loss function for smooth animations, and a text-conditioning mechanism that enables fine-grained control over character movements and expressions. The system can generate diverse animation sequences from simple text descriptions like "character walking" or "character dancing".</p>
        </div>
        <div class="image-slideshow">
          <div class="slideshow-container">
            <div class="slide active">
              <img src="gallery/research/4d-mesh/image1.jpg" alt="4D Mesh Animation 1">
              <div class="slide-caption">4D mesh character animation framework</div>
            </div>
            <div class="slide">
              <img src="gallery/research/4d-mesh/image2.jpg" alt="GNN Architecture">
              <div class="slide-caption">Graph Neural Network architecture design</div>
            </div>
            <div class="slide">
              <img src="gallery/research/4d-mesh/image3.jpg" alt="Diffusion Results">
              <div class="slide-caption">Diffusion model results and comparisons</div>
            </div>
          </div>
          <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">❮</button>
          <button class="slideshow-nav next" onclick="changeSlide(this, 1)">❯</button>
          <div class="slideshow-dots">
            <span class="dot active" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
          </div>
        </div>
      </div>
    </div>

    <div class="item expandable-item" data-project="nnu-net">
      <div class="item-header">
        <div class="item-title-content">
          <h3>Lightweight Student Network for nnU‑Net (in progress)</h3>
          <div class="sub">Amrita Vishwa Vidyapeetham &mdash; Sep 2025 – Present</div>
          <ul>
            <li>Multi‑stage compression with feature + soft‑label distillation and deep supervision.</li>
            <li>Target: clinically deployable nnU‑Net with reduced parameters/memory/latency.</li>
          </ul>
        </div>
        <i class="fa-solid fa-chevron-down expand-icon"></i>
      </div>
      <div class="expanded-content">
        <div class="project-details">
          <h4>Clinical Motivation</h4>
          <p>The nnU-Net framework has achieved state-of-the-art performance in medical image segmentation but requires significant computational resources, limiting its deployment in clinical settings. This research aims to develop a lightweight student network that maintains clinical accuracy while being deployable on resource-constrained medical devices and edge computing platforms.</p>
        </div>
        <div class="project-details">
          <h4>Knowledge Distillation Strategy</h4>
          <p>Our multi-stage compression approach combines feature-level distillation, soft-label knowledge transfer, and deep supervision techniques. The student network learns from intermediate representations of the teacher nnU-Net, enabling it to capture essential anatomical features while reducing computational complexity by 10x.</p>
        </div>
        <div class="project-details">
          <h4>Clinical Impact</h4>
          <p>The lightweight model targets real-time medical image analysis in point-of-care settings, enabling faster diagnosis and treatment planning. Initial results show 90% parameter reduction while maintaining 95% of the original segmentation accuracy across multiple medical imaging modalities including CT, MRI, and ultrasound.</p>
        </div>
        <div class="image-slideshow">
          <div class="slideshow-container">
            <div class="slide active">
              <img src="gallery/research/nnu-net/image1.jpg" alt="nnU-Net Compression">
              <div class="slide-caption">nnU-Net compression methodology</div>
            </div>
            <div class="slide">
              <img src="gallery/research/nnu-net/image2.jpg" alt="Knowledge Distillation">
              <div class="slide-caption">Knowledge distillation framework</div>
            </div>
            <div class="slide">
              <img src="gallery/research/nnu-net/image3.jpg" alt="Performance Metrics">
              <div class="slide-caption">Performance metrics and clinical validation</div>
            </div>
          </div>
          <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">❮</button>
          <button class="slideshow-nav next" onclick="changeSlide(this, 1)">❯</button>
          <div class="slideshow-dots">
            <span class="dot active" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
          </div>
        </div>
      </div>
    </div>

    <div class="item expandable-item" data-project="self-driving">
      <div class="item-header">
        <div class="item-title-content">
          <h3>Final Year Project — Self‑Driving Cars with Small Language Models</h3>
          <div class="sub">Amrita Vishwa Vidyapeetham &mdash; Sep 2025 – Present</div>
          <ul>
            <li>Developing lightweight autonomous driving solutions using Qwen‑0.5B with multimodal encoders (LiDAR + cameras).</li>
            <li>Designing pipelines for real‑time waypoint prediction, scene understanding, and object detection on edge hardware.</li>
          </ul>
        </div>
        <i class="fa-solid fa-chevron-down expand-icon"></i>
      </div>
      <div class="expanded-content">
        <div class="project-details">
          <h4>Project Vision</h4>
          <p>This final year project explores the integration of small language models (SLMs) in autonomous driving systems to enable more interpretable and efficient decision-making. Using Qwen-0.5B as the core reasoning engine, the system combines multimodal sensor data to make driving decisions that can be explained in natural language.</p>
        </div>
        <div class="project-details">
          <h4>Multimodal Architecture</h4>
          <p>The system integrates LiDAR point clouds and camera imagery through specialized encoders that feed into the small language model. Custom tokenization schemes represent spatial and temporal driving data as text tokens, enabling the language model to reason about complex driving scenarios and generate both waypoints and natural language explanations.</p>
        </div>
        <div class="project-details">
          <h4>Edge Computing Focus</h4>
          <p>Designed specifically for edge deployment, the system optimizes model quantization and pruning techniques to achieve real-time performance on automotive-grade hardware. The pipeline includes efficient sensor fusion algorithms, optimized inference loops, and fail-safe mechanisms for safe autonomous operation.</p>
        </div>
        <div class="image-slideshow">
          <div class="slideshow-container">
            <div class="slide active">
              <img src="gallery/research/self-driving/image1.jpg" alt="Autonomous Car System">
              <div class="slide-caption">Complete autonomous driving system overview</div>
            </div>
            <div class="slide">
              <img src="gallery/research/self-driving/image2.jpg" alt="LiDAR + Camera Fusion">
              <div class="slide-caption">LiDAR and camera sensor fusion pipeline</div>
            </div>
            <div class="slide">
              <img src="gallery/research/self-driving/image3.jpg" alt="Real-time Detection">
              <div class="slide-caption">Real-time object detection and tracking</div>
            </div>
          </div>
          <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">❮</button>
          <button class="slideshow-nav next" onclick="changeSlide(this, 1)">❯</button>
          <div class="slideshow-dots">
            <span class="dot active" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Projects -->
      <section id="projects" class="container fade-up" style="animation-delay:0.6s;">
    <h2><i class="fa-solid fa-code"></i> Projects</h2>
    <div class="item">
      <h3>Deep Fake Detection</h3>
      <div class="sub">Jan 2024 – Apr 2024</div>
      <ul>
        <li>Developed a video deepfake detection system leveraging multiple detectors with unique methodologies.</li>
        <li>Built an ensemble framework where results were intelligently combined by a meta‑model based on detectors’ historical performance.</li>
      </ul>
    </div>
    <div class="item">
      <h3>DDPM Image Generation</h3>
      <div class="sub">2024</div>
      <ul>
        <li>Implemented a Denoising Diffusion Probabilistic Model (DDPM) for image synthesis as part of deep learning coursework.</li>
        <li>Showcased diffusion‑based generative modeling and its applications in media synthesis and AI ethics demonstrations.</li>
      </ul>
    </div>
    <div class="item">
      <h3>Adobe India Hackathon — Team Starks (Connecting the Dots)</h3>
      <div class="sub">Jan 2025</div>
      <ul>
        <li>Built a lightweight CPU‑only offline system to transform static PDFs into dynamic, structured, persona‑aware knowledge artifacts.</li>
        <li>Integrated quantized language models with object detection and semantic search to enable retrieval and summarization.</li>
      </ul>
    </div>
    <div class="item">
      <h3>Fire Fighting Drone for Early Forest Fire Detection and Extinguishment</h3>
      <div class="sub">2018</div>
      <ul>
        <li>Developed a drone capable of early forest fire detection and suppression with integrated surveillance and rapid response mechanisms.</li>
        <li>Won multiple awards: CBSE Science Fair State Finalist, PPTIA Innovation Award National Finalist (Top 10), First Prize at Shastra Science Fair.</li>
      </ul>
    </div>
  </section>

  <!-- Skills -->
      <section id="skills" class="container fade-up" style="animation-delay:0.8s;">
    <h2><i class="fa-solid fa-wrench"></i> Skills</h2>
    <div class="chips">
      <span class="chip">Python</span>
      <span class="chip">Java</span>
      <span class="chip">C++</span>
      <span class="chip">C</span>
      <span class="chip">PyTorch</span>
      <span class="chip">PyTorch3D</span>
      <span class="chip">TensorFlow</span>
      <span class="chip">Scikit‑Learn</span>
      <span class="chip">NumPy</span>
      <span class="chip">Pandas</span>
      <span class="chip">Matplotlib</span>
      <span class="chip">Seaborn</span>
      <span class="chip">MediaPipe</span>
      <span class="chip">OpenCV</span>
      <span class="chip">AWS</span>
      <span class="chip">Prompt Engineering</span>
      <span class="chip">Model Fine‑tuning</span>
      <span class="chip">Quantization</span>
      <span class="chip">Knowledge Distillation</span>
      <span class="chip">CUDA</span>
    </div>
  </section>

  <!-- Education -->
      <section id="education" class="container fade-up" style="animation-delay:1s;">
    <h2><i class="fa-solid fa-graduation-cap"></i> Education</h2>
    <div class="item">
      <h3>B.Tech in Computer Science &amp; Engineering</h3>
      <div class="sub">Amrita Vishwa Vidyapeetham, Coimbatore &mdash; 2022–2026</div>
      <ul>
        <li>CGPA: 8.41 (till 6th semester)</li>
      </ul>
    </div>
    <div class="item">
      <h3>Senior Secondary (CBSE)</h3>
      <div class="sub">St. Peter’s Senior Secondary School &mdash; 2022</div>
      <ul>
        <li>Percentage: 86.2%</li>
      </ul>
    </div>
    <div class="item">
      <h3>Higher Secondary (CBSE)</h3>
      <div class="sub">St. Peter’s Senior Secondary School &mdash; 2020</div>
      <ul>
        <li>Percentage: 89.8%</li>
      </ul>
    </div>
  </section>

  <!-- Contact -->
      <section id="contact" class="container fade-up" style="animation-delay:1.2s;">
    <h2><i class="fa-solid fa-envelope"></i> Contact</h2>
    <div class="item">
      <ul>
        <li><i class="fa-solid fa-envelope" style="color:#10b981; margin-right:0.4rem;"></i> <a href="mailto:bmaheshraj23@gmail.com">bmaheshraj23@gmail.com</a></li>
        <li><i class="fa-solid fa-phone" style="color:#10b981; margin-right:0.4rem;"></i> <a href="tel:+918921662922">+91&#8209;8921662922</a></li>
        <li><i class="fa-brands fa-github" style="color:#10b981; margin-right:0.4rem;"></i> <a href="https://github.com/maheXh" target="_blank" rel="noopener">maheXh</a></li>
        <li><i class="fa-brands fa-linkedin" style="color:#10b981; margin-right:0.4rem;"></i> <a href="https://www.linkedin.com" target="_blank" rel="noopener">LinkedIn</a></li>
        <li><i class="fa-solid fa-download" style="color:#10b981; margin-right:0.4rem;"></i> <a href="resume.pdf" download>Download Resume</a></li>
      </ul>
    </div>
  </section>

  <!-- Footer -->
  <footer class="container">
    <div>&copy; <span id="year"></span> B. Mahesh Raj</div>
    <div class="footer-links">
      <a href="https://github.com/maheXh" target="_blank" rel="noopener"><i class="fa-brands fa-github"></i> GitHub</a>
      <a href="https://www.linkedin.com" target="_blank" rel="noopener"><i class="fa-brands fa-linkedin"></i> LinkedIn</a>
      <a href="#about">Back to top</a>
    </div>
  </footer>
  <script>
    // Set current year in footer
    document.getElementById('year').textContent = new Date().getFullYear();

    // Expandable items functionality
    document.addEventListener('DOMContentLoaded', function() {
      // Add click event listeners to expandable items
      const expandableItems = document.querySelectorAll('.expandable-item');
      
      expandableItems.forEach(item => {
        const header = item.querySelector('.item-header');
        const expandIcon = item.querySelector('.expand-icon');
        
        header.addEventListener('click', function() {
          // Toggle expanded state
          item.classList.toggle('expanded');
          
          // Update expand icon
          if (item.classList.contains('expanded')) {
            expandIcon.style.transform = 'rotate(180deg)';
          } else {
            expandIcon.style.transform = 'rotate(0deg)';
          }
        });
      });

      // Initialize slideshows
      initializeSlideshows();
    });

    // Slideshow functionality
    function initializeSlideshows() {
      const slideshows = document.querySelectorAll('.image-slideshow');
      
      slideshows.forEach(slideshow => {
        const slides = slideshow.querySelectorAll('.slide');
        const dots = slideshow.querySelectorAll('.dot');
        let currentSlide = 0;

        // Auto-advance slides every 5 seconds
        setInterval(() => {
          if (slideshow.closest('.expanded')) {
            currentSlide = (currentSlide + 1) % slides.length;
            showSlide(slideshow, currentSlide);
          }
        }, 5000);
      });
    }

    function changeSlide(button, direction) {
      const slideshow = button.closest('.image-slideshow');
      const slides = slideshow.querySelectorAll('.slide');
      const dots = slideshow.querySelectorAll('.dot');
      
      // Find current slide
      let currentSlide = 0;
      slides.forEach((slide, index) => {
        if (slide.classList.contains('active')) {
          currentSlide = index;
        }
      });
      
      // Calculate next slide
      currentSlide += direction;
      if (currentSlide >= slides.length) currentSlide = 0;
      if (currentSlide < 0) currentSlide = slides.length - 1;
      
      showSlide(slideshow, currentSlide);
    }

    function currentSlide(dot, slideIndex) {
      const slideshow = dot.closest('.image-slideshow');
      showSlide(slideshow, slideIndex - 1);
    }

    function showSlide(slideshow, slideIndex) {
      const slides = slideshow.querySelectorAll('.slide');
      const dots = slideshow.querySelectorAll('.dot');
      
      // Hide all slides and remove active from dots
      slides.forEach(slide => slide.classList.remove('active'));
      dots.forEach(dot => dot.classList.remove('active'));
      
      // Show selected slide and activate corresponding dot
      slides[slideIndex].classList.add('active');
      dots[slideIndex].classList.add('active');
    }
  </script>
</body>
</html>